---
layout: about
title: About
start_sents: ðŸ‘‹ Hi! I'm Hetong Wang. 
permalink: /
subtitle: # <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: hetong_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: <center><a href="mailto:hetong.wang809@gmail.com"><p>hetong.wang809@gmail.com</a></p></center>
# <p>123 your address street</p>
# <p>Your City, State 12345</p>

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
My name is çŽ‹é¹¤ç«¥ in Chinese. Feel free to call me `Hetong` or `Erika` alternatively! 

I play with data, structure, and model behavior concurrently to explore mechanistic interpretability. Recently, I have been particularly excited about the following questions:

* **<u>Sparsity</u>**: How is information clustered and dispersed among the neurons of models? How do different training paradigms steer this distribution, and how do the circuits formed between neurons contribute to the generalisation performance?
* **<u>Training Dynamics</u>**: How is knowledge acquired, and how does it evolve under varying data mixtures and learning curricula?

I believe that addressing these questions can lead to greater **data and parameter efficiency** while allowing us to reclaim more control from the black-box nature of neural networks. 

<!-- Besides, I am interested in explaining deep learning through the lens of [information theory](https://en.wikipedia.org/wiki/Information_theory), such as [information bottleneck](https://en.wikipedia.org/wiki/Information_bottleneck_method). -->


<!-- My ultimate research goal is to build a general-purpose model that could go beyond imitation and simple memorization, i.e. perform as intelligent agents, who can learn new strategies from few but essential data, e.g. task description and instruction, rules and interaction with the environment. 

Meanwhile, I found **(1) the interpretability of deep neural models**, **(2) data and parameter efficient skill discovery and alignment methods**, **(3) cross-lingual and Out-Of-Distribution(OOD) generalisation**, are interesting topics that could lead me towards the grand goal. 

Moreover, formulating and explaining deep learning through the lens of [information theory](https://en.wikipedia.org/wiki/Information_theory) attracted and inspired me a lot, I am excited to discuss this topic with folks who interested as well! -->

### About

I am currently a research intern at [TsinghuaNLP](https://nlp.csai.tsinghua.edu.cn/), under the supervision of Prof. [Zhiyuan Liu](https://nlp.csai.tsinghua.edu.cn/~lzy/). I obtained my master in [Artificial Intelligence](https://www.ed.ac.uk/studying/postgraduate/degrees/index.php?r=site/view&edition=2024&id=107) at [The Univerisity of Edinburgh](https://www.ed.ac.uk/), where I am fortunately advised by Prof. [Edoardo Ponti](https://ducdauge.github.io) and Dr. [Pasquale Minervini](https://neuralnoise.com). Before that, I obtained my bachelor in [Computer Science](https://www.liverpool.ac.uk/courses/2024/computer-science-bsc-hons) at [The University of Liverpool](https://www.liverpool.ac.uk/), with a research focus on Reinforcement Learning.

<!-- , working closely with Prof. [Gabriella Pizzuto](https://gabriellapizzuto.github.io/) under the supervision of Prof. [Andy Cooper](https://www.liverpool.ac.uk/chemistry/staff/andrew-cooper/) -->